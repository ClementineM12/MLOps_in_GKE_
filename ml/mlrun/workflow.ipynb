{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Pipeline\n",
    "\n",
    "This pipeline performs the following steps:\n",
    "1. **Fetch Dataset:** Check if the dataset exists in MinIO (or download it from Kaggle if not), then upload it.\n",
    "2. **Process Data:** Load the metadata CSV, merge image paths, and perform some plotting.\n",
    "3. **Feature Engineering:** Perform feature engineering on the processed metadata.\n",
    "4. **Train Random Forest Model:** Train a Random Forest model on the engineered dataset.\n",
    "5. **Train CNN Model:** Train a CNN on a sample of the engineered dataset.\n",
    "\n",
    "Below is the complete pipeline implementation using MLRun in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Project\n",
    "Create a project using mlrun.get_or_create_project (make sure to load it in case it already exists), creating the paths where we'll store the project's artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "\n",
    "# Set our project's name:\n",
    "project_name = \"mlop\"\n",
    "project_dir = os.path.abspath('./')\n",
    "\n",
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(project_name, project_dir, user_project=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A project in MLRun is based on the MLRun Functions it can run. In this notebook we will see two ways to create a MLRun Function:\n",
    "\n",
    "* `mlrun.code_to_function`: Create our own MLRun Function from code (will be used for training and evaluation in section 4).\n",
    "* `mlrun.import_function`: Import from [MLRun's functions marketplace](https://www.mlrun.org/hub/) - a [functions hub](https://docs.mlrun.org/en/v1.1.2/runtimes/load-from-marketplace.html) intended to be a centralized location for open source contributions of function components (will be used for downloading the data in section 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build project images\n",
    "Building the images to satisfy the requirements of the project, according to the selection above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "project.build_image(\n",
    "    image=f\".mlop-base\",\n",
    "    base_image='mlrun/mlrun',\n",
    "    commands=[\n",
    "        'pip install kagglehub',\n",
    "        'pip install tensorflow~=2.16.0',\n",
    "        'pip install minio',\n",
    "        'pip install pandas',\n",
    "        'pip install matplotlib==3.5.3',\n",
    "        'pip install seaborn',\n",
    "        'pip install scikit-learn',\n",
    "        'pip install opencv-python-headless',\n",
    "        'pip install Pillow',\n",
    "        'pip install numpy',\n",
    "    ],\n",
    "    extra_args=\"--skip-tls-verify\",\n",
    "    overwrite_build_params=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the MLRun Function & Run\n",
    "We will use MLRun's `mlrun.code_to_function` to create a MLRun Function from our code in the above mentioned python file. \n",
    "\n",
    "We wish to run the load first as a Job, so we will set the kind parameter to \"job\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata_filename = \"HAM10000_metadata.csv\" \n",
    "source_bucket = \"data\" \n",
    "processed_bucket = \"processed-data\"\n",
    "processed_metadata_filename = \"processed_metadata.pkl\"\n",
    "images_dir = \"images\"\n",
    "\n",
    "mem_label = {\"dedicated\": \"highmem\"}\n",
    "cpu_label = {\"dedicated\": \"highcpu\"}\n",
    "\n",
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create the function parsing the given file code using 'code_to_function':\n",
    "load_function = mlrun.code_to_function(\n",
    "    filename=\"functions/load.py\",\n",
    "    name=\"mlop-base\",\n",
    "    kind=\"job\",\n",
    "    image='.load-data'\n",
    ")\n",
    "\n",
    "load_function.with_node_selection(node_selector=mem_label)\n",
    "\n",
    "load_run = load_function.run(\n",
    "    name=\"Load data into Minio Object Storage\",\n",
    "    handler=\"fetch_dataset\",\n",
    "    params={\n",
    "        \"metadata_filename\": metadata_filename,\n",
    "        \"bucket_name\": source_bucket, \n",
    "        \"images_dir\": images_dir,\n",
    "    },\n",
    "    local=False\n",
    ")\n",
    "\n",
    "# Wait for complition and show the results. \n",
    "load_run.wait_for_completion()\n",
    "load_run.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_data_fn = project.set_function(\n",
    "    \"functions/load.py\",\n",
    "    name=\"load-data\",\n",
    "    kind=\"job\",\n",
    "    image='.mlop-base'\n",
    ")\n",
    "processing_fn = project.set_function(\n",
    "    \"functions/process.py\",\n",
    "    name=\"processing\",\n",
    "    kind=\"job\",\n",
    "    image='.mlop-base'\n",
    ")\n",
    "feature_engineering_fn = project.set_function(\n",
    "    \"functions/feature_engineering.py\",\n",
    "    name=\"feature-engineering\",\n",
    "    kind=\"job\",\n",
    "    image='.mlop-base'\n",
    ")\n",
    "model_fn = project.set_function(\n",
    "    \"functions/model.py\",\n",
    "    name=\"model\",\n",
    "    kind=\"job\",\n",
    "    image='.mlop-base'\n",
    ")\n",
    "\n",
    "load_data_fn.with_node_selection(node_selector=mem_label)\n",
    "processing_fn.with_node_selection(node_selector=mem_label)\n",
    "feature_engineering_fn.with_node_selection(node_selector=mem_label)\n",
    "model_fn.with_node_selection(node_selector=cpu_label)\n",
    "\n",
    "mlrun.mlconf.is_ce_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile workflow.py\n",
    "import mlrun\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from typing import Literal\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"Skin Cancer Detection Pipeline\")\n",
    "def kfpipeline(\n",
    "    target_model: Literal[\"rf\", \"cnn\"],\n",
    "    segmented_samples: int = 2000,\n",
    "    sample: int =400,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 10,\n",
    "):     \n",
    "    # Defaults\n",
    "    metadata_filename = \"HAM10000_metadata.csv\" \n",
    "    source_bucket = \"data\" \n",
    "    processed_bucket = \"processed-data\"\n",
    "    processed_metadata_filename = \"processed_metadata.pkl\"\n",
    "    images_dir = \"images\"\n",
    "    # Get the project object\n",
    "    project = mlrun.get_current_project()\n",
    "\n",
    "    load_run = mlrun.run_function(\n",
    "        name=\"load-data-into-minio-object-storage\",\n",
    "        function=\"load-data\",\n",
    "        handler=\"fetch_dataset\",\n",
    "        params={\n",
    "            \"metadata_filename\": metadata_filename,\n",
    "            \"bucket_name\": source_bucket, \n",
    "            \"images_dir\": images_dir,\n",
    "        },\n",
    "        local=False\n",
    "    )\n",
    "\n",
    "    process_run = mlrun.run_function(\n",
    "        name=\"preprocess-data\",\n",
    "        function=\"processing\",\n",
    "        handler=\"process_metadata\",\n",
    "        params={\n",
    "            \"metadata_filename\": metadata_filename,\n",
    "            \"source_bucket\": source_bucket, \n",
    "            \"processed_bucket\": processed_bucket,\n",
    "            \"images_dir\": images_dir,\n",
    "            \"processed_metadata_filename\": processed_metadata_filename,\n",
    "        },\n",
    "        local=False\n",
    "    ).after(load_run)\n",
    "    \n",
    "    create_segmented_run = mlrun.run_function(\n",
    "        name=\"preprocess-images\",\n",
    "        function=\"processing\",\n",
    "        handler=\"create_segmented_images\",\n",
    "        params={\n",
    "            \"processed_bucket\": processed_bucket,\n",
    "            \"processed_metadata_filename\": processed_metadata_filename,\n",
    "            \"n_samples\": segmented_samples,\n",
    "        },\n",
    "        local=False,\n",
    "        watch=True,  # <- Turn on the logs.\n",
    "    ).after(process_run)\n",
    "\n",
    "    feature_engineering_run = mlrun.run_function(\n",
    "        name=\"feature-engineering\",\n",
    "        function=\"feature-engineering\",\n",
    "        handler=\"feature_engineer\",\n",
    "        params={\n",
    "            \"processed_bucket\": processed_bucket,\n",
    "            \"processed_metadata_filename\": processed_metadata_filename,\n",
    "        },\n",
    "        local=False,\n",
    "        watch=True,  # <- Turn on the logs.\n",
    "    ).after(create_segmented_run)\n",
    "\n",
    "    with dsl.Condition(target_model == \"rf\"): \n",
    "        model_run = mlrun.run_function(\n",
    "            name=\"train-random-forest-model\",\n",
    "            function=\"model\",\n",
    "            handler=\"train_random_forest\",\n",
    "            params={\n",
    "                \"processed_bucket\": processed_bucket,\n",
    "                \"processed_metadata_filename\": processed_metadata_filename,\n",
    "            },\n",
    "            local=False,\n",
    "            watch=True,  # <- Turn on the logs.\n",
    "        ).after(feature_engineering_run)\n",
    "\n",
    "    with dsl.Condition(target_model == \"cnn\"): \n",
    "        training_cnn_run = mlrun.run_function(\n",
    "            name=\"train-cnn\",\n",
    "            function=\"model\",\n",
    "            handler=\"train_cnn\",\n",
    "            params={\n",
    "                \"processed_bucket\": processed_bucket,\n",
    "                \"processed_metadata_filename\": processed_metadata_filename,\n",
    "                \"sample\": sample,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": epochs,\n",
    "            },\n",
    "            local=False\n",
    "        ).after(feature_engineering_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Register the workflow file:\n",
    "workflow_name = \"skin_cancer_detection_workflow\"\n",
    "project.set_workflow(workflow_name, \"workflow.py\")\n",
    "\n",
    "# Save the project:\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "project.run(\n",
    "    name=workflow_name,\n",
    "    arguments={\n",
    "        \"target_model\": \"cnn\",\n",
    "        \"sample\": 1000,\n",
    "    },\n",
    "    watch=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
